{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/profmcnich/example_notebook/blob/main/a3_sample_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports section\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Temperature °C  Mols KCL     Size nm^3\n",
      "0              469       647  6.244743e+05\n",
      "1              403       694  5.779610e+05\n",
      "2              302       975  6.196847e+05\n",
      "3              779       916  1.460449e+06\n",
      "4              901        18  4.325726e+04\n",
      "5              545       637  7.124634e+05\n",
      "6              660       519  7.006960e+05\n",
      "7              143       869  2.718260e+05\n",
      "8               89       461  8.919803e+04\n",
      "9              294       776  4.770210e+05\n",
      "10             991       117  2.441771e+05\n",
      "11             307       781  5.006455e+05\n",
      "12             206        70  3.145200e+04\n",
      "13             437       599  5.390215e+05\n",
      "14             566        75  9.185271e+04\n",
      "\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Temperature °C  1000 non-null   int64  \n",
      " 1   Mols KCL        1000 non-null   int64  \n",
      " 2   Size nm^3       1000 non-null   float64\n",
      "dtypes: float64(1), int64(2)\n",
      "memory usage: 23.6 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Using pandas load the dataset (load remotely, not locally)\n",
    "slime = pd.read_csv(\"https://raw.githubusercontent.com/profmcnich/example_notebook/main/science_data_large.csv\")\n",
    "\n",
    "# Output the first 15 rows of the data\n",
    "print(slime.head(15), end=\"\\n\\n\\n\\n\")\n",
    "\n",
    "# Display a summary of the table information (number of datapoints, etc.)\n",
    "print(slime.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the pandas dataset and split it into our features (X) and label (y)\n",
    "features_x = slime.iloc[0:, 0:2]\n",
    "label_y = slime.iloc[0:, 2]\n",
    "\n",
    "\n",
    "# Use sklearn to split the features and labels into a training/test set. (90% train, 10% test)\n",
    "x_train, x_test, y_train, y_test = train_test_split(features_x, label_y, train_size = 0.9, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Perform a Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  [ 5.34930333e+05  7.51519290e+05  9.09896431e+05 -7.52177430e+04\n",
      "  1.20991155e+05  1.07579946e+05  1.37872683e+05  1.10612883e+06\n",
      " -3.25850143e+05  5.07193250e+05  4.58242351e+05  8.72064674e+05\n",
      "  5.87855573e+05  8.70565404e+05 -4.08534222e+05  3.37124454e+05\n",
      "  1.53491969e+04  4.02081158e+05  3.14131990e+05  3.46744326e+05\n",
      "  7.83588661e+05  4.50526373e+05  1.56042617e+05  9.88814657e+04\n",
      "  2.94637729e+05  2.26672057e+05 -2.81874551e+05  4.01689634e+05\n",
      "  4.39223220e+05  1.15973202e+05  1.15891573e+06  4.87438754e+05\n",
      "  8.01301930e+05  4.97067013e+05 -4.00615768e+05  8.64860098e+05\n",
      "  8.87421415e+05  2.01728695e+05  4.25506787e+05  4.73950313e+05\n",
      " -1.22410144e+05  9.88025268e+05 -1.60478962e+05  5.28332856e+05\n",
      "  1.18885757e+06  7.23172086e+05  7.98150262e+05  5.01568198e+05\n",
      "  3.33053410e+05  1.92104793e+05  1.00336286e+06 -2.73956097e+05\n",
      "  3.76157310e+05  4.22917555e+05  8.48947974e+05  5.97856893e+05\n",
      "  1.05072985e+05  7.25506119e+05  1.14654176e+06 -5.29777722e+04\n",
      "  6.53014425e+05  7.05189186e+05  5.18804650e+05  6.50720015e+05\n",
      "  7.33858744e+05  5.27305693e+05  7.05790239e+05  1.42170552e+06\n",
      "  1.10959983e+06  3.38416211e+05  6.40679071e+05  5.42685936e+05\n",
      "  5.54500491e+05  8.65717356e+05  7.58820243e+05  2.71992127e+05\n",
      "  8.36628741e+05  8.84693842e+05 -2.78940472e+05  8.49245817e+05\n",
      "  5.45395372e+05  3.37362523e+05  7.56241087e+05  8.07073385e+05\n",
      "  6.44184656e+05  6.40566246e+05  1.18460856e+06 -9.01708680e+04\n",
      "  4.18414355e+05  1.20869262e+05  2.21442560e+05  1.00993785e+06\n",
      "  1.33173862e+06  3.92467332e+05  3.96071872e+02  2.97947211e+05\n",
      "  7.40257776e+05  4.82829782e+05  1.23290334e+06  6.92860885e+05]\n"
     ]
    }
   ],
   "source": [
    "# Use sklearn to train a model on the training set\n",
    "reg = LinearRegression()\n",
    "model = reg.fit(x_train, y_train)\n",
    "\n",
    "# Create a sample datapoint and predict the output of that sample with the trained model\n",
    "predictions = reg.predict(x_test)\n",
    "print(\"Predictions: \", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 value:  0.8606652835638592\n"
     ]
    }
   ],
   "source": [
    "# Report on the score for that model, in your own words (markdown, not code) explain what the score means\n",
    "print(\"R^2 value: \",reg.score(features_x, label_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does the score mean? \n",
    "- R-squared is a goodness-of-fit measure for linear regression models. Basically, it means that this model has certain percentage (for this model, it is 86%) accuracy predicting the size of slime based on temperature and mol variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficents:  [ 883.78301691 1025.1479957 ]\n",
      "intercept:  -416311.3108023002\n",
      "coefficents:  883.7830169093772\n",
      "coefficents:  1025.1479956990834\n"
     ]
    }
   ],
   "source": [
    "# Extract the coefficents and intercept from the model and write an equation for your h(x) using LaTeX\n",
    "print(\"coefficents: \", reg.coef_)\n",
    "print(\"intercept: \", reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample equation: $E = mc^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. Use Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.83918826, 0.87051239, 0.85871066, 0.87202623, 0.84364641])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the cross_val_score function to repeat your experiment across many shuffles of the data\n",
    "scores = cross_val_score(LinearRegression(), features_x, label_y)\n",
    "\n",
    "# Report on their finding and their significance\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5. Using Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[139 726 467 782 235 780 303 461 272  51 615 419 395 729 246 621 535 829\n",
      " 253 255 827 855 734 610 113 281 815 491 326 268 153 659 418 549  49 301\n",
      " 635 205 317 846 302 889 464  74 868 543  68 771 254 332 774 587 708 217\n",
      " 533 186 360 348 732 309  64 745 243 859 805 754 432   7 484 536  18 789\n",
      "  65 459 107  66   3 839 229  67 356 714 232 689 522 155  86 251 489 444\n",
      " 684 741  52 170 405  56 537 256 645 711 258 690 541 446 646 355 649 834\n",
      " 407   4 600 843 480 144 803 560 331 860 273 441 682 125 437 231 497 620\n",
      " 826 147 445 219 168 266 540 313 490 694 263 790  38 160 809 492 733 278\n",
      " 642   9 455 233 676 297 722 575  93 765 248  76 882  75 470 758 193 452\n",
      " 823 704 828 213 763 323 717 201 698 388 850 894 523 657 752 551 479 772\n",
      " 381 270 568 794  71 239 230 700 545 408 604 861 555  46 358 468 596 788\n",
      " 330 881 857 148 299 259 528 202   2 154 117  98 664 718 220 324 366 319\n",
      " 801 149 798 118 496 538 542 163 180 411 312 181 176 389 482 890 475 612\n",
      "  12 166 792 314 507 637 667 385 577  96  41 179 835 295 415 674 586 271\n",
      " 341 398 234 370 878 158 353 289 517 553 378 367 845 837 761  11 723 364\n",
      " 849 174 450  19 588 487 363 836 875 376 884 211 504 393 833 421 685 242\n",
      " 404 521 721 661 576 329 893 438  88  90 663 888 420  87 634 146 580  39\n",
      " 628  28  43 123 383 237 548 699 673 570 736 173 115  55 105 701 238 776\n",
      " 379 430 529 629 284 579 249 397 677 558 557 111 195  32 177 622 185 578\n",
      " 848 775 349 137 402 767 108 183 675 178 410 493 807  25 406 640 184 897\n",
      " 352 136 127 285 390 506 164 680 787 785  73 669 375 550 141 526 668 735\n",
      " 275 825 876  48 386 871 465 582 106 755 244 508 783 252 643 730 618 670\n",
      " 644 434 206  10 883 494 753 671 412 679 311 100 473 122 716 112 428 296\n",
      "  89 267 633 334 509 463 581 597 374 810 567 851 292 725 727 274 250 705\n",
      "  82 624 218 167 196  57 631 686 357 606 651 365 126 485 257 351 226 703\n",
      " 262 719 279 354 887 327 609 743 638 261 328 477  26 709  77 737 786 422\n",
      "  78 867 865 277 298 121 595 639 531 583 563 387 377 287 454 527 400 142\n",
      " 795 200 822 891 416 647 161 514 469 619 757  54 157 806 571 140 110 524\n",
      " 476 189 222 288 460 832 414  47 143 336 569 623 626  58 350 325  69 852\n",
      " 802 747  80  91 731  83 503 744 221 692 796 413 895 276 191 530 706 636\n",
      " 641 498 423 474 594 800 885 182 207 564 712 770   6  63 742 630 870 691\n",
      " 856 320 318  24  34 863  22 858 872  60 625 372 724 584 321 260 282 340\n",
      " 488 812 779 793 165 554 655 892 844 611 481 562 344 572 874 172 458 486\n",
      "  40  45 768 429 840 648 457 159  13  94 520 198 688 525 269 573 652 818\n",
      " 672 510 862   0 539 209 462 665 819  59 760 854  81 453 898 778 877 264\n",
      " 841 101 135  14 821 215 589 306 346 162   1 660 199 132 552   8 707 607\n",
      "  72 104 585 435 627 203 602 304 308 740 392  33  62 399  21 192 808 451\n",
      " 286 683 138 869  53 394 204 847 283 133 109 280 483 212 811 791 129 513\n",
      "  61 187  29 294 119 762 864 838 150 764 322 693 124 547 766 561 500 134\n",
      " 728 702 499 409 361 873  95 443 546 208 315 715 175 156 391 603 820 359\n",
      " 599 293 369 247  35 784 310 401 120 516 265 214 830 814 681 169 245 656\n",
      " 442 608 591 592 879 632 518 152 216 190 345 574 617 687  70 756 425  30\n",
      " 816 804  92 307 720 305 566 502 511 436 396 472 225 382 746 417 426  17\n",
      " 102 759 713 853 605 478 447 373 653 678 556 103 750 797 866 194 210 748\n",
      " 773 616 342 368 739  37 371 532 380 650 403 145 896 300 886 601 197 515\n",
      " 769 658 880 738 512 424 236 290 654 439 799 116 710  85  99 227  15 559\n",
      " 666 433  97 471 291 696 613 427 130 188  79 565 449 384 519 223 534 598\n",
      " 456 131 831 593 240 749  23 128  16  27 842 333  50 114 466 505 337 151\n",
      " 614 899 544 662 440 777  44 362 316 695 339 590 751 817  36 697  31  84\n",
      " 781 338 335 241 224 501 813  20 448 431 495 343  42 347 171   5 228 824]\n",
      "continuous\n",
      "multiclass\n",
      "multiclass\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kazis\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of continuous and multiclass targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-7993fe45e492>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mlogistic_regression\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_scores_encoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mlogistic_regression\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    498\u001b[0m         \"\"\"\n\u001b[0;32m    499\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;31m# Compute accuracy for each possible representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'multilabel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0m\u001b[0;32m     93\u001b[0m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous and multiclass targets"
     ]
    }
   ],
   "source": [
    "# Using the PolynomialFeatures library perform another regression on an augmented dataset of degree 2\n",
    "\n",
    "# logistic_regression = LogisticRegression()\n",
    "# logistic_regression.fit(x_train, y_train)\n",
    "# logistic_regression.score(x_test, y_test)\n",
    "\n",
    "# Report on the metrics and output the resultant equation as you did in Part 3.\n",
    "\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "training_scores_encoded = lab_enc.fit_transform(y_train)\n",
    "print(training_scores_encoded)\n",
    "print(utils.multiclass.type_of_target(y_train))\n",
    "print(utils.multiclass.type_of_target(y_train.astype('int')))\n",
    "print(utils.multiclass.type_of_target(training_scores_encoded))\n",
    "\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(x_train, training_scores_encoded)\n",
    "logistic_regression.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn                        import metrics, svm\n",
    "from sklearn.linear_model           import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "training_data_X    = np.array([ [1.2, 6.7, 2.7],  [2.3, 4.6, 2.2],  [0.3, 3.9, 0.8],  [2.1, 1.3, 4.3]  ])\n",
    "training_scores_Y  = np.array( [1.4, 9.2, 2.5, 2.2] )\n",
    "prediction_data_test  = np.array([ [1.5, 3.4, 2.2],  [7.6, 7.2, 0.2] ])\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(training_data_X, training_scores_Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_enc = preprocessing.LabelEncoder()\n",
    "training_scores_encoded = lab_enc.fit_transform(training_scores_Y)\n",
    "print(training_scores_encoded)\n",
    "print(utils.multiclass.type_of_target(training_scores_Y))\n",
    "print(utils.multiclass.type_of_target(training_scores_Y.astype('int')))\n",
    "print(utils.multiclass.type_of_target(training_scores_encoded))\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(training_data_X, training_scores_encoded)\n",
    "print(\"LogisticRegression\")\n",
    "print(clf.predict(prediction_data_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
